{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba171e22-6ce9-472c-8bcc-54e9de653378",
   "metadata": {},
   "source": [
    "Assignment - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb8537-0597-4a59-849d-26785b569a5e",
   "metadata": {},
   "source": [
    "Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0e88ed-ba1d-40f4-92f9-49a40d90b113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/10 17:57:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import explode, split, col, desc\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import shutil\n",
    "import os\n",
    "import string\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "output_path_1 = \"output_1\"\n",
    "\n",
    "# Remove the output directories if they already exist\n",
    "if os.path.exists(output_path_1):\n",
    "    shutil.rmtree(output_path_1)\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"WordCountApp\").master(\"local[*]\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d198d29-b82d-413a-b343-979d64c1323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|value             |\n",
      "+------------------+\n",
      "|('jo', 1293)      |\n",
      "|('said', 1245)    |\n",
      "|('one', 1159)     |\n",
      "|('mr', 1123)      |\n",
      "|('little', 961)   |\n",
      "|('would', 929)    |\n",
      "|('could', 893)    |\n",
      "|('much', 704)     |\n",
      "|('like', 676)     |\n",
      "|('meg', 653)      |\n",
      "|('mrs', 606)      |\n",
      "|('never', 605)    |\n",
      "|('elizabeth', 601)|\n",
      "|('amy', 588)      |\n",
      "|('see', 574)      |\n",
      "|('good', 572)     |\n",
      "|('laurie', 564)   |\n",
      "|('well', 557)     |\n",
      "|('know', 557)     |\n",
      "|('dont', 552)     |\n",
      "|('time', 522)     |\n",
      "|('go', 501)       |\n",
      "|('think', 496)    |\n",
      "|('must', 462)     |\n",
      "|('away', 453)     |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    text_file = sc.textFile(\"littlewoman.txt\")\n",
    "    text_file_2 = sc.textFile(\"pride_and_prejudice.txt\")\n",
    "    \n",
    "    count_combined = text_file.union(text_file_2)\n",
    "    count_combined_transformation = count_combined.flatMap(lambda line: line.translate(str.maketrans(\"\", \"\", string.punctuation)).lower().split())\n",
    "    count_combined_filter= count_combined_transformation.filter(lambda word: word not in stop_words)\n",
    "    count_combined_mapping = count_combined_filter.map(lambda word: (word, 1))\n",
    "    count_combined_unique = count_combined_mapping.reduceByKey(lambda a, b: a + b)\n",
    "    count_combined_sorted = count_combined_unique.sortBy(lambda x: x[1],ascending=False)\n",
    "\n",
    "    count_combined_sorted.saveAsTextFile(output_path_1)\n",
    "\n",
    "    # Merge partition files into a single output file\n",
    "\n",
    "    with open(\"output_1.txt\", \"w\") as outfile:\n",
    "        for filename in sorted(os.listdir(output_path_1)):\n",
    "            if filename.startswith(\"part-\"):\n",
    "                with open(os.path.join(output_path_1, filename), \"r\") as infile:\n",
    "                    outfile.write(infile.read())\n",
    "    \n",
    "    text_df = spark.read.text(\"output_1.txt\")\n",
    "\n",
    "    top_25_words = text_df.limit(25)\n",
    "    top_25_words.show(25, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "     print(f\"An error occurred: {e}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86216853-ada6-40b6-bd8f-f49978431181",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea79af1e-1d55-4d1c-a02c-ac1c5f4a2902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/10 17:58:36 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 18:===========================================>            (17 + 5) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page with highest rank: ('50', 4.176179975971073)\n",
      "Page with lowest rank: ('35', 0.2593061945644343)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PageRank\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load and parse the network file\n",
    "lines = sc.textFile(\"question3.txt\")\n",
    "\n",
    "# Parse each line into (page, neighbors) pairs\n",
    "def parse_neighbors(line):\n",
    "    parts = re.split(r':\\s*\\[|\\]', line)\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    page = parts[0].strip()\n",
    "    neighbors = parts[1].strip().split(', ')\n",
    "    return page, neighbors\n",
    "\n",
    "# Create an RDD of (page, list of neighbors)\n",
    "links = lines.map(parse_neighbors).filter(lambda x: x is not None)\n",
    "\n",
    "# Initialize each page's rank to 1.0\n",
    "ranks = links.mapValues(lambda _: 1.0)\n",
    "\n",
    "# Number of iterations for convergence\n",
    "iterations = 10\n",
    "damping_factor = 0.85  # Damping factor for PageRank\n",
    "\n",
    "# Run PageRank algorithm for a fixed number of iterations\n",
    "for _ in range(iterations):\n",
    "    # Calculate contributions for each page\n",
    "    contributions = links.join(ranks).flatMap(\n",
    "        lambda page_neighbors_rank: [(neighbor, page_neighbors_rank[1][1] / len(page_neighbors_rank[1][0])) \n",
    "                                     for neighbor in page_neighbors_rank[1][0]]\n",
    "    )\n",
    "    \n",
    "    # Calculate new ranks based on contributions\n",
    "    ranks = contributions.reduceByKey(lambda a, b: a + b).mapValues(\n",
    "        lambda rank: (1 - damping_factor) + damping_factor * rank\n",
    "    )\n",
    "\n",
    "# Collect and save the final ranks to output file\n",
    "page_ranks = ranks.collect()\n",
    "with open(\"output_page_ranks.txt\", \"w\") as f:\n",
    "    for page, rank in page_ranks:\n",
    "        f.write(f\"{page}: {rank}\\n\")\n",
    "\n",
    "# Find the page with the highest and lowest PageRank\n",
    "max_page = max(page_ranks, key=lambda x: x[1])\n",
    "min_page = min(page_ranks, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Page with highest rank: {max_page}\")\n",
    "print(f\"Page with lowest rank: {min_page}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d489ecc-407f-496e-9903-2ab332a88a39",
   "metadata": {},
   "source": [
    "Djikstra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32cf0f17-712b-4876-892f-9faf252f3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with greatest distance from 0 (Distance: 3.0): [32, 2, 35, 11, 13, 15, 51, 20, 90]\n",
      "Nodes with least distance from 0 (Distance: 1.0): [1, 66, 6, 7, 39, 71, 40, 9, 41, 75, 43, 76, 14, 16, 49, 19, 53, 54, 87, 57, 28, 60, 92]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import explode, split, col, desc\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkContext\n",
    "spark = SparkSession.builder.appName(\"Djikstra\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# Read the data files\n",
    "edges_rdd = sc.textFile('question2_1.txt').union(sc.textFile('question2_2.txt'))\n",
    "\n",
    "# Parse the edges into (source_node, destination_node, weight)\n",
    "edges = edges_rdd.map(lambda line: line.strip().split(',')) \\\n",
    "                 .map(lambda parts: (int(parts[0]), int(parts[1]), float(parts[2])))\n",
    "\n",
    "# Create an adjacency list RDD: (node, list of (neighbor, weight))\n",
    "adjacency_list = edges.map(lambda x: (x[0], (x[1], x[2]))) \\\n",
    "                      .groupByKey() \\\n",
    "                      .mapValues(list) \\\n",
    "                      .cache()\n",
    "\n",
    "# Get all nodes\n",
    "nodes_from = edges.map(lambda x: x[0])\n",
    "nodes_to = edges.map(lambda x: x[1])\n",
    "all_nodes = nodes_from.union(nodes_to).distinct().cache()\n",
    "\n",
    "# Initialize distances: (node, distance)\n",
    "start_node = 0  # Assuming the first node is 0\n",
    "infinity = float('inf')\n",
    "distances = all_nodes.map(lambda node: (node, infinity))\n",
    "distances = distances.map(lambda x: (x[0], 0.0) if x[0] == start_node else x)\n",
    "distances = distances.cache()\n",
    "\n",
    "# Iterative update of distances\n",
    "updated = True\n",
    "iteration = 0\n",
    "max_iterations = all_nodes.count() - 1  # Maximum possible iterations\n",
    "\n",
    "while updated and iteration < max_iterations:\n",
    "    iteration += 1\n",
    "    # Join distances with adjacency list\n",
    "    joined = distances.join(adjacency_list, numPartitions=8)\n",
    "    \n",
    "    # Compute tentative distances\n",
    "    tentative_distances = joined.flatMap(lambda x: [ \n",
    "        (neighbor[0], x[1][0] + neighbor[1]) for neighbor in x[1][1]\n",
    "    ])\n",
    "    \n",
    "    # Combine the new distances with the existing ones\n",
    "    new_distances = distances.union(tentative_distances) \\\n",
    "                             .reduceByKey(lambda x, y: min(x, y))\n",
    "    \n",
    "    # Check if distances have changed\n",
    "    changes = new_distances.join(distances).filter(lambda x: x[1][0] != x[1][1])\n",
    "    updated = not changes.isEmpty()\n",
    "    \n",
    "    # Update distances for the next iteration\n",
    "    distances = new_distances\n",
    "    distances = distances.cache()\n",
    "\n",
    "# Collect final distances\n",
    "final_distances = distances.collectAsMap()\n",
    "\n",
    "# Write distances to output file\n",
    "with open('output_2.txt', 'w') as f:\n",
    "    for node in sorted(final_distances.keys()):\n",
    "        dist = final_distances[node]\n",
    "        if dist == infinity:\n",
    "            f.write(f\"{node} unreachable\\n\")\n",
    "        else:\n",
    "            f.write(f\"{node} {dist}\\n\")\n",
    "\n",
    "# Find nodes with greatest and least distances (excluding infinity and the start node)\n",
    "reachable_nodes = {node: dist for node, dist in final_distances.items() if dist != infinity and node != start_node}\n",
    "if reachable_nodes:\n",
    "    # Find the maximum and minimum distances\n",
    "    max_distance = max(reachable_nodes.values())\n",
    "    min_distance = min(reachable_nodes.values())\n",
    "    \n",
    "    # Find all nodes that have the maximum and minimum distances\n",
    "    max_nodes = [node for node, dist in reachable_nodes.items() if dist == max_distance]\n",
    "    min_nodes = [node for node, dist in reachable_nodes.items() if dist == min_distance]\n",
    "    \n",
    "    # Print nodes with greatest distance\n",
    "    print(f\"Nodes with greatest distance from {start_node} (Distance: {max_distance}): {max_nodes}\")\n",
    "    \n",
    "    # Print nodes with least distance\n",
    "    print(f\"Nodes with least distance from {start_node} (Distance: {min_distance}): {min_nodes}\")\n",
    "else:\n",
    "    print(\"No reachable nodes from the starting node.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfbe723-4a5d-4716-b781-d1c0ce77f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b237dd10-7e3e-496d-92fc-ae2f01206069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/10 17:56:04 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/10 17:56:04 ERROR SparkContext: Error initializing SparkContext.\n",
      "org.apache.spark.SparkException: Could not parse Master URL: '['local[*]']'\n",
      "\tat org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:3194)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:577)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/10 17:56:04 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Could not parse Master URL: '['local[*]']'\n\tat org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:3194)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:577)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize Spark session\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPageRank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load and parse the network file\u001b[39;00m\n",
      "File \u001b[0;32m~/ub1/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/ub1/.venv/lib/python3.12/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/ub1/.venv/lib/python3.12/site-packages/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/ub1/.venv/lib/python3.12/site-packages/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/ub1/.venv/lib/python3.12/site-packages/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ub1/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/ub1/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/ub1/.venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Could not parse Master URL: '['local[*]']'\n\tat org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:3194)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:577)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PageRank\").master([\"local[*]\"]).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load and parse the network file\n",
    "lines = sc.textFile(\"question3.txt\")\n",
    "\n",
    "# Parse each line into (page, neighbors) pairs\n",
    "def parse_neighbors(line):\n",
    "    parts = re.split(r':\\s*\\[|\\]', line)\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    page = parts[0].strip()\n",
    "    neighbors = parts[1].strip().split(', ')\n",
    "    return page, neighbors\n",
    "\n",
    "# Create an RDD of (page, list of neighbors)\n",
    "links = lines.map(parse_neighbors).filter(lambda x: x is not None)\n",
    "\n",
    "# Initialize each page's rank to 1.0\n",
    "ranks = links.mapValues(lambda _: 1.0)\n",
    "\n",
    "# Number of iterations for convergence\n",
    "iterations = 10\n",
    "damping_factor = 0.85  # Damping factor for PageRank\n",
    "\n",
    "# Run PageRank algorithm for a fixed number of iterations\n",
    "for _ in range(iterations):\n",
    "    # Calculate contributions for each page\n",
    "    contributions = links.join(ranks).flatMap(\n",
    "        lambda page_neighbors_rank: [(neighbor, page_neighbors_rank[1][1] / len(page_neighbors_rank[1][0])) \n",
    "                                     for neighbor in page_neighbors_rank[1][0]]\n",
    "    )\n",
    "    \n",
    "    # Calculate new ranks based on contributions\n",
    "    ranks = contributions.reduceByKey(lambda a, b: a + b).mapValues(\n",
    "        lambda rank: (1 - damping_factor) + damping_factor * rank\n",
    "    )\n",
    "\n",
    "# Collect and save the final ranks to output file\n",
    "page_ranks = ranks.collect()\n",
    "with open(\"output_page_ranks.txt\", \"w\") as f:\n",
    "    for page, rank in page_ranks:\n",
    "        f.write(f\"{page}: {rank}\\n\")\n",
    "\n",
    "# Find the page with the highest and lowest PageRank\n",
    "max_page = max(page_ranks, key=lambda x: x[1])\n",
    "min_page = min(page_ranks, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Page with highest rank: {max_page}\")\n",
    "print(f\"Page with lowest rank: {min_page}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518d851-8157-4304-bbc2-1f26c0eae875",
   "metadata": {},
   "source": [
    "Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdffe6a-624f-4758-92e8-6015d7c62b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

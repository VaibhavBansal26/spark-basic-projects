{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba171e22-6ce9-472c-8bcc-54e9de653378",
   "metadata": {},
   "source": [
    "Assignment - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb8537-0597-4a59-849d-26785b569a5e",
   "metadata": {},
   "source": [
    "Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d0e88ed-ba1d-40f4-92f9-49a40d90b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import explode, split, col, desc\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import shutil\n",
    "import os\n",
    "import string\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "output_path_1 = \"output_1\"\n",
    "\n",
    "# Remove the output directories if they already exist\n",
    "if os.path.exists(output_path_1):\n",
    "    shutil.rmtree(output_path_1)\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"WordCountApp\").master(\"local[*]\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d198d29-b82d-413a-b343-979d64c1323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|value             |\n",
      "+------------------+\n",
      "|('jo', 1293)      |\n",
      "|('said', 1245)    |\n",
      "|('one', 1159)     |\n",
      "|('mr', 1123)      |\n",
      "|('little', 961)   |\n",
      "|('would', 929)    |\n",
      "|('could', 893)    |\n",
      "|('much', 704)     |\n",
      "|('like', 676)     |\n",
      "|('meg', 653)      |\n",
      "|('mrs', 606)      |\n",
      "|('never', 605)    |\n",
      "|('elizabeth', 601)|\n",
      "|('amy', 588)      |\n",
      "|('see', 574)      |\n",
      "|('good', 572)     |\n",
      "|('laurie', 564)   |\n",
      "|('well', 557)     |\n",
      "|('know', 557)     |\n",
      "|('dont', 552)     |\n",
      "|('time', 522)     |\n",
      "|('go', 501)       |\n",
      "|('think', 496)    |\n",
      "|('must', 462)     |\n",
      "|('away', 453)     |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    text_file = sc.textFile(\"littlewoman.txt\")\n",
    "    text_file_2 = sc.textFile(\"pride_and_prejudice.txt\")\n",
    "    \n",
    "    count_combined = text_file.union(text_file_2)\n",
    "    count_combined_transformation = count_combined.flatMap(lambda line: line.translate(str.maketrans(\"\", \"\", string.punctuation)).lower().split())\n",
    "    count_combined_filter= count_combined_transformation.filter(lambda word: word not in stop_words)\n",
    "    count_combined_mapping = count_combined_filter.map(lambda word: (word, 1))\n",
    "    count_combined_unique = count_combined_mapping.reduceByKey(lambda a, b: a + b)\n",
    "    count_combined_sorted = count_combined_unique.sortBy(lambda x: x[1],ascending=False)\n",
    "\n",
    "    count_combined_sorted.saveAsTextFile(output_path_1)\n",
    "\n",
    "    # Merge partition files into a single output file\n",
    "\n",
    "    with open(\"output_1.txt\", \"w\") as outfile:\n",
    "        for filename in sorted(os.listdir(output_path_1)):\n",
    "            if filename.startswith(\"part-\"):\n",
    "                with open(os.path.join(output_path_1, filename), \"r\") as infile:\n",
    "                    outfile.write(infile.read())\n",
    "    \n",
    "    text_df = spark.read.text(\"output_1.txt\")\n",
    "\n",
    "    top_25_words = text_df.limit(25)\n",
    "    top_25_words.show(25, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "     print(f\"An error occurred: {e}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86216853-ada6-40b6-bd8f-f49978431181",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "32cf0f17-712b-4876-892f-9faf252f3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with greatest distance from 0 (Distance: 3.0): [32, 2, 35, 11, 13, 15, 51, 20, 90]\n",
      "Nodes with least distance from 0 (Distance: 1.0): [1, 66, 6, 7, 39, 71, 40, 9, 41, 75, 43, 76, 14, 16, 49, 19, 53, 54, 87, 57, 28, 60, 92]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import explode, split, col, desc\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkContext\n",
    "spark = SparkSession.builder.appName(\"Djikstra\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# Read the data files\n",
    "edges_rdd = sc.textFile('question2_1.txt').union(sc.textFile('question2_2.txt'))\n",
    "\n",
    "# Parse the edges into (source_node, destination_node, weight)\n",
    "edges = edges_rdd.map(lambda line: line.strip().split(',')) \\\n",
    "                 .map(lambda parts: (int(parts[0]), int(parts[1]), float(parts[2])))\n",
    "\n",
    "# Create an adjacency list RDD: (node, list of (neighbor, weight))\n",
    "adjacency_list = edges.map(lambda x: (x[0], (x[1], x[2]))) \\\n",
    "                      .groupByKey() \\\n",
    "                      .mapValues(list) \\\n",
    "                      .cache()\n",
    "\n",
    "# Get all nodes\n",
    "nodes_from = edges.map(lambda x: x[0])\n",
    "nodes_to = edges.map(lambda x: x[1])\n",
    "all_nodes = nodes_from.union(nodes_to).distinct().cache()\n",
    "\n",
    "# Initialize distances: (node, distance)\n",
    "start_node = 0  # Assuming the first node is 0\n",
    "infinity = float('inf')\n",
    "distances = all_nodes.map(lambda node: (node, infinity))\n",
    "distances = distances.map(lambda x: (x[0], 0.0) if x[0] == start_node else x)\n",
    "distances = distances.cache()\n",
    "\n",
    "# Iterative update of distances\n",
    "updated = True\n",
    "iteration = 0\n",
    "max_iterations = all_nodes.count() - 1  # Maximum possible iterations\n",
    "\n",
    "while updated and iteration < max_iterations:\n",
    "    iteration += 1\n",
    "    # Join distances with adjacency list\n",
    "    joined = distances.join(adjacency_list, numPartitions=8)\n",
    "    \n",
    "    # Compute tentative distances\n",
    "    tentative_distances = joined.flatMap(lambda x: [ \n",
    "        (neighbor[0], x[1][0] + neighbor[1]) for neighbor in x[1][1]\n",
    "    ])\n",
    "    \n",
    "    # Combine the new distances with the existing ones\n",
    "    new_distances = distances.union(tentative_distances) \\\n",
    "                             .reduceByKey(lambda x, y: min(x, y))\n",
    "    \n",
    "    # Check if distances have changed\n",
    "    changes = new_distances.join(distances).filter(lambda x: x[1][0] != x[1][1])\n",
    "    updated = not changes.isEmpty()\n",
    "    \n",
    "    # Update distances for the next iteration\n",
    "    distances = new_distances\n",
    "    distances = distances.cache()\n",
    "\n",
    "# Collect final distances\n",
    "final_distances = distances.collectAsMap()\n",
    "\n",
    "# Write distances to output file\n",
    "with open('output_2.txt', 'w') as f:\n",
    "    for node in sorted(final_distances.keys()):\n",
    "        dist = final_distances[node]\n",
    "        if dist == infinity:\n",
    "            f.write(f\"{node} unreachable\\n\")\n",
    "        else:\n",
    "            f.write(f\"{node} {dist}\\n\")\n",
    "\n",
    "# Find nodes with greatest and least distances (excluding infinity and the start node)\n",
    "reachable_nodes = {node: dist for node, dist in final_distances.items() if dist != infinity and node != start_node}\n",
    "if reachable_nodes:\n",
    "    # Find the maximum and minimum distances\n",
    "    max_distance = max(reachable_nodes.values())\n",
    "    min_distance = min(reachable_nodes.values())\n",
    "    \n",
    "    # Find all nodes that have the maximum and minimum distances\n",
    "    max_nodes = [node for node, dist in reachable_nodes.items() if dist == max_distance]\n",
    "    min_nodes = [node for node, dist in reachable_nodes.items() if dist == min_distance]\n",
    "    \n",
    "    # Print nodes with greatest distance\n",
    "    print(f\"Nodes with greatest distance from {start_node} (Distance: {max_distance}): {max_nodes}\")\n",
    "    \n",
    "    # Print nodes with least distance\n",
    "    print(f\"Nodes with least distance from {start_node} (Distance: {min_distance}): {min_nodes}\")\n",
    "else:\n",
    "    print(\"No reachable nodes from the starting node.\")\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23743e9e-409d-4b01-be32-b2031269f29e",
   "metadata": {},
   "source": [
    "Implement Djikstras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c0f8ee2-b72d-4bf8-8c28-a6220035f0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node with greatest distance from start node 0: ((3, inf), inf)\n",
      "Node with least distance from start node 0 (excluding itself): ((3, inf), inf)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Initialize Spark context\n",
    "spark = SparkSession.builder.appName(\"Djikstra\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# Define input files\n",
    "file_path1 = \"question2_1.txt\"\n",
    "file_path2 = \"question2_2.txt\"\n",
    "\n",
    "# Define output path\n",
    "output_path = \"output_2.txt\"\n",
    "\n",
    "# Remove output file if it already exists\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "# Helper function to safely parse lines\n",
    "def process_edge(line):\n",
    "    try:\n",
    "        src, dest, weight = map(int, line.split(\", \"))\n",
    "        return src, dest, weight\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing line '{line}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Load data from both files, handle parsing errors\n",
    "edges = sc.textFile(file_path1).union(sc.textFile(file_path2)) \\\n",
    "           .map(process_edge) \\\n",
    "           .filter(lambda x: x is not None)  # Remove any failed parses\n",
    "\n",
    "# Start node for Dijkstra's algorithm\n",
    "start_node = 0\n",
    "\n",
    "# Initialize distances with the start node set to 0 and others to infinity\n",
    "distances = edges.flatMap(lambda edge: [(edge[0], float('inf')), (edge[1], float('inf'))]) \\\n",
    "                 .distinct() \\\n",
    "                 .map(lambda node: (node, 0 if node == start_node else float('inf')))\n",
    "\n",
    "# Function to perform an iteration of Dijkstra's algorithm\n",
    "def update_distances(distances, edges):\n",
    "    updated_distances = distances.join(edges) \\\n",
    "                                 .map(lambda x: (x[1][1], x[1][0] + x[1][2]))  # (destination, new_distance)\n",
    "    new_distances = updated_distances.reduceByKey(min)\n",
    "    # Merge new distances with the old ones, keeping the minimum distance for each node\n",
    "    return distances.fullOuterJoin(new_distances) \\\n",
    "                    .mapValues(lambda x: min([v for v in x if v is not None]))\n",
    "\n",
    "# Iteratively apply Dijkstra's algorithm until no changes occur\n",
    "converged = False\n",
    "while not converged:\n",
    "    # Update distances\n",
    "    new_distances = update_distances(distances, edges)\n",
    "\n",
    "    # Check if distances have changed by comparing the RDDs\n",
    "    changes = distances.join(new_distances) \\\n",
    "                       .filter(lambda x: x[1][0] != x[1][1]) \\\n",
    "                       .isEmpty()\n",
    "    \n",
    "    # Update distances for the next iteration\n",
    "    distances = new_distances\n",
    "\n",
    "    # If there are no changes, the algorithm has converged\n",
    "    converged = changes\n",
    "\n",
    "# Collect final distances\n",
    "final_distances = distances.collect()\n",
    "\n",
    "# Save results to output file\n",
    "with open(output_path, \"w\") as f:\n",
    "    for node, distance in sorted(final_distances):\n",
    "        f.write(f\"Node {node} has distance {distance}\\n\")\n",
    "\n",
    "# Find nodes with the greatest and least distance from the start node\n",
    "greatest_distance_node = max(final_distances, key=lambda x: x[1])\n",
    "least_distance_node = min([item for item in final_distances if item[1] != 0], key=lambda x: x[1])\n",
    "\n",
    "print(f\"Node with greatest distance from start node {start_node}: {greatest_distance_node}\")\n",
    "print(f\"Node with least distance from start node {start_node} (excluding itself): {least_distance_node}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2be3f1-afc2-4c26-8665-2a7bb24264ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 16:09:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 41:===============================>                        (25 + 8) / 44]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node with greatest distance: ('1,', inf)\n",
      "Node with least distance: ('1,', inf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DijkstraShortestPath\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Define a large number to represent infinity\n",
    "INFINITY = float('inf')\n",
    "\n",
    "# Load graph edges from both files\n",
    "edges1 = sc.textFile(\"question2_1.txt\")\n",
    "edges2 = sc.textFile(\"question2_2.txt\")\n",
    "\n",
    "# Parse each line into (source, destination, weight)\n",
    "edges = edges1.union(edges2).map(lambda line: line.split()).map(lambda parts: (parts[0], parts[1], float(parts[2])))\n",
    "\n",
    "# Specify the starting node\n",
    "start_node = 'A'  # Replace 'A' with the desired starting node\n",
    "\n",
    "# Initialize distances RDD with start node distance 0, others as infinity\n",
    "distances = edges.flatMap(lambda x: [(x[0], INFINITY), (x[1], INFINITY)]) \\\n",
    "                 .distinct() \\\n",
    "                 .map(lambda x: (x[0], 0 if x[0] == start_node else INFINITY))\n",
    "\n",
    "# Initialize edges RDD for graph representation\n",
    "graph = edges.map(lambda x: (x[0], (x[1], x[2])))\n",
    "\n",
    "# Iteratively update distances using Dijkstra's algorithm\n",
    "def dijkstra_update(distances, graph):\n",
    "    updated_distances = distances.join(graph) \\\n",
    "        .flatMap(lambda x: [(x[0], x[1][0]), (x[1][1][0], x[1][0] + x[1][1][1])]) \\\n",
    "        .reduceByKey(lambda a, b: min(a, b))\n",
    "    return updated_distances\n",
    "\n",
    "for _ in range(10):  # Limit iterations for convergence; adjust as necessary\n",
    "    distances = dijkstra_update(distances, graph)\n",
    "\n",
    "# Collect and save results to output_2.txt\n",
    "shortest_paths = distances.collect()\n",
    "with open(\"output_3.txt\", \"w\") as f:\n",
    "    for node, dist in shortest_paths:\n",
    "        f.write(f\"{node}: {dist}\\n\")\n",
    "\n",
    "# Find nodes with the greatest and least distance\n",
    "max_node = max(shortest_paths, key=lambda x: x[1])\n",
    "min_node = min(shortest_paths, key=lambda x: x[1] if x[1] > 0 else sys.maxsize)\n",
    "\n",
    "print(f\"Node with greatest distance: {max_node}\")\n",
    "print(f\"Node with least distance: {min_node}\")\n",
    "\n",
    "# Stop Spark session\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518d851-8157-4304-bbc2-1f26c0eae875",
   "metadata": {},
   "source": [
    "Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b237dd10-7e3e-496d-92fc-ae2f01206069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 16:10:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 60:================================================>       (19 + 3) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page with highest rank: ('50', 4.176179975971073)\n",
      "Page with lowest rank: ('35', 0.2593061945644343)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PageRank\").master([\"local[*]\"]).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load and parse the network file\n",
    "lines = sc.textFile(\"question3.txt\")\n",
    "\n",
    "# Parse each line into (page, neighbors) pairs\n",
    "def parse_neighbors(line):\n",
    "    parts = re.split(r':\\s*\\[|\\]', line)\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    page = parts[0].strip()\n",
    "    neighbors = parts[1].strip().split(', ')\n",
    "    return page, neighbors\n",
    "\n",
    "# Create an RDD of (page, list of neighbors)\n",
    "links = lines.map(parse_neighbors).filter(lambda x: x is not None)\n",
    "\n",
    "# Initialize each page's rank to 1.0\n",
    "ranks = links.mapValues(lambda _: 1.0)\n",
    "\n",
    "# Number of iterations for convergence\n",
    "iterations = 10\n",
    "damping_factor = 0.85  # Damping factor for PageRank\n",
    "\n",
    "# Run PageRank algorithm for a fixed number of iterations\n",
    "for _ in range(iterations):\n",
    "    # Calculate contributions for each page\n",
    "    contributions = links.join(ranks).flatMap(\n",
    "        lambda page_neighbors_rank: [(neighbor, page_neighbors_rank[1][1] / len(page_neighbors_rank[1][0])) \n",
    "                                     for neighbor in page_neighbors_rank[1][0]]\n",
    "    )\n",
    "    \n",
    "    # Calculate new ranks based on contributions\n",
    "    ranks = contributions.reduceByKey(lambda a, b: a + b).mapValues(\n",
    "        lambda rank: (1 - damping_factor) + damping_factor * rank\n",
    "    )\n",
    "\n",
    "# Collect and save the final ranks to output file\n",
    "page_ranks = ranks.collect()\n",
    "with open(\"output_page_ranks.txt\", \"w\") as f:\n",
    "    for page, rank in page_ranks:\n",
    "        f.write(f\"{page}: {rank}\\n\")\n",
    "\n",
    "# Find the page with the highest and lowest PageRank\n",
    "max_page = max(page_ranks, key=lambda x: x[1])\n",
    "min_page = min(page_ranks, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Page with highest rank: {max_page}\")\n",
    "print(f\"Page with lowest rank: {min_page}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdffe6a-624f-4758-92e8-6015d7c62b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

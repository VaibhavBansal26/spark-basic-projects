{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba171e22-6ce9-472c-8bcc-54e9de653378",
   "metadata": {},
   "source": [
    "# Assignment - 2\n",
    "\n",
    "1. Word Count\n",
    "2. Extended Word Count\n",
    "3. Page Rank Algorithm\n",
    "4. Dijkstra's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb8537-0597-4a59-849d-26785b569a5e",
   "metadata": {},
   "source": [
    "# Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d0e88ed-ba1d-40f4-92f9-49a40d90b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import explode, split, col, desc\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import shutil\n",
    "import os\n",
    "import string\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "output_path_1 = \"output_1\"\n",
    "\n",
    "# Remove the output directories if they already exist\n",
    "if os.path.exists(output_path_1):\n",
    "    shutil.rmtree(output_path_1)\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"WordCountApp\").master(\"local[*]\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58500ec4-1298-43fc-98ce-d1590504ff40",
   "metadata": {},
   "source": [
    "# 1.Normal Word Count Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d198d29-b82d-413a-b343-979d64c1323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load text files\n",
    "    text_file_1 = sc.textFile(\"littlewoman.txt\")\n",
    "    text_file_2 = sc.textFile(\"pride_and_prejudice.txt\")\n",
    "    \n",
    "    # Combine both text files\n",
    "    count_combined = text_file_1.union(text_file_2)\n",
    "    \n",
    "\n",
    "    count_combined_basic = count_combined.flatMap(lambda line: line.split())\n",
    "    count_combined_mapping = count_combined_basic.map(lambda word: (word, 1))\n",
    "    count_combined_unique = count_combined_mapping.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # Save basic word count output\n",
    "    output_path_1 = \"output_1\"\n",
    "    count_combined_unique.saveAsTextFile(output_path_1)\n",
    "    \n",
    "    # Merge partition files into a single file\n",
    "    with open(\"output_1.txt\", \"w\") as outfile:\n",
    "        for filename in sorted(os.listdir(output_path_1)):\n",
    "            if filename.startswith(\"part-\"):\n",
    "                with open(os.path.join(output_path_1, filename), \"r\") as infile:\n",
    "                    outfile.write(infile.read())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86216853-ada6-40b6-bd8f-f49978431181",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1cfec-16c1-4543-9768-3eb1796cd4b6",
   "metadata": {},
   "source": [
    "# 2. Extened Word Count Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49139d75-2d55-4110-a074-479dea52b022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Word Count Extended\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "try:\n",
    "    # Load the text files again\n",
    "    text_file_1 = sc.textFile(\"littlewoman.txt\")\n",
    "    text_file_2 = sc.textFile(\"pride_and_prejudice.txt\")\n",
    "    \n",
    "    # Combine both text files\n",
    "    count_combined = text_file_1.union(text_file_2)\n",
    "    \n",
    "    # Transformations\n",
    "    count_combined_transformation = count_combined.flatMap(lambda line: line.translate(str.maketrans(\"\", \"\", string.punctuation))\\\n",
    "                                                           .lower().split())\n",
    "    count_combined_filtered = count_combined_transformation.filter(lambda word: word not in stop_words)\n",
    "    \n",
    "    # Mapping and Counting\n",
    "    count_combined_mapping = count_combined_filtered.map(lambda word: (word, 1))\n",
    "    count_combined_reduced = count_combined_mapping.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # Sorting in descending order\n",
    "    count_combined_sorted = count_combined_reduced.sortBy(lambda x: x[1], ascending=False)\n",
    "    _\n",
    "    output_path_extended = \"output_1_extended\"\n",
    "    count_combined_sorted.saveAsTextFile(output_path_extended)\n",
    "    \n",
    "    # Merge partition files into a single output file for transformed word count\n",
    "    with open(\"output_1_extended.txt\", \"w\") as outfile:\n",
    "        for filename in sorted(os.listdir(output_path_extended)):\n",
    "            if filename.startswith(\"part-\"):\n",
    "                with open(os.path.join(output_path_extended, filename), \"r\") as infile:\n",
    "                    outfile.write(infile.read())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5474363-df8e-41d0-b66b-ae9394f7c83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|value             |\n",
      "+------------------+\n",
      "|('jo', 1293)      |\n",
      "|('said', 1245)    |\n",
      "|('one', 1159)     |\n",
      "|('mr', 1123)      |\n",
      "|('little', 961)   |\n",
      "|('would', 929)    |\n",
      "|('could', 893)    |\n",
      "|('much', 704)     |\n",
      "|('like', 676)     |\n",
      "|('meg', 653)      |\n",
      "|('mrs', 606)      |\n",
      "|('never', 605)    |\n",
      "|('elizabeth', 601)|\n",
      "|('amy', 588)      |\n",
      "|('see', 574)      |\n",
      "|('good', 572)     |\n",
      "|('laurie', 564)   |\n",
      "|('well', 557)     |\n",
      "|('know', 557)     |\n",
      "|('dont', 552)     |\n",
      "|('time', 522)     |\n",
      "|('go', 501)       |\n",
      "|('think', 496)    |\n",
      "|('must', 462)     |\n",
      "|('away', 453)     |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = spark.read.text(\"output_1_extended.txt\")\n",
    "top_25_words = text_df.limit(25)\n",
    "top_25_words.show(25, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bceb4c0c-4f89-4431-b38f-67937468763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e251f31-6aac-4b4a-a07c-700fba3cdfa0",
   "metadata": {},
   "source": [
    "# 3. Page Rank Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea79af1e-1d55-4d1c-a02c-ac1c5f4a2902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=================================================>      (28 + 4) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page with highest rank: ('50', 4.176468539659126)\n",
      "Page with lowest rank: ('35', 0.2592591391247631)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PageRank\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "lines = sc.textFile(\"question3.txt\")\n",
    "\n",
    "def parse_neighbors(line):\n",
    "    parts = re.split(r':\\s*\\[|\\]', line)\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    page = parts[0].strip()\n",
    "    neighbors = parts[1].strip().split(', ')\n",
    "    return page, neighbors\n",
    "\n",
    "# Create an RDD of (page, list of neighbors)\n",
    "links = lines.map(parse_neighbors).filter(lambda x: x is not None)\n",
    "\n",
    "# Initialize each page's rank to 1.0\n",
    "ranks = links.mapValues(lambda _: 1.0)\n",
    "\n",
    "iterations = 15 # Number of iterations for convergence\n",
    "damping_factor = 0.85  # Damping factor for PageRank\n",
    "\n",
    "# PageRank algorithm for a fixed number of iterations\n",
    "for _ in range(iterations):\n",
    "    contributions = links.join(ranks).flatMap(\n",
    "        lambda page_neighbors_rank: [(neighbor, page_neighbors_rank[1][1] / len(page_neighbors_rank[1][0])) \n",
    "                                     for neighbor in page_neighbors_rank[1][0]]\n",
    "    )\n",
    "    \n",
    "    # Calculate new ranks\n",
    "    ranks = contributions.reduceByKey(lambda a, b: a + b).mapValues(\n",
    "        lambda rank: (1 - damping_factor) + damping_factor * rank\n",
    "    )\n",
    "page_ranks = ranks.collect()\n",
    "with open(\"output_page_ranks.txt\", \"w\") as f:\n",
    "    for page, rank in page_ranks:\n",
    "        f.write(f\"{page}: {rank}\\n\")\n",
    "\n",
    "# Find the page with the highest and lowest PageRank\n",
    "max_page = max(page_ranks, key=lambda x: x[1])\n",
    "min_page = min(page_ranks, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Page with highest rank: {max_page}\")\n",
    "print(f\"Page with lowest rank: {min_page}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b58fc449-a68c-4312-a7e8-3134640ad184",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518d851-8157-4304-bbc2-1f26c0eae875",
   "metadata": {},
   "source": [
    "# 4. Dijkstra's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cdffe6a-624f-4758-92e8-6015d7c62b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with greatest distance from 0 (Distance: 14.0): [15]\n",
      "Nodes with least distance from 0 (Distance: 2.0): [16]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dijkstra\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "edges_rdd = sc.textFile('question2_1.txt').union(sc.textFile('question2_2.txt'))\n",
    "\n",
    "# Parse the edges into (source_node, destination_node, weight)\n",
    "# summing their weights\n",
    "edges = edges_rdd.map(lambda line: line.strip().split(',')) \\\n",
    "                 .map(lambda parts: ((int(parts[0]), int(parts[1])), float(parts[2]))) \\\n",
    "                 .reduceByKey(lambda w1, w2: w1 + w2) \\\n",
    "                 .map(lambda x: (x[0][0], x[0][1], x[1]))\n",
    "\n",
    "# Create an adjacency list RDD\n",
    "adjacency_list = edges.map(lambda x: (x[0], (x[1], x[2]))) \\\n",
    "                      .groupByKey() \\\n",
    "                      .mapValues(list) \\\n",
    "                      .cache()\n",
    "\n",
    "nodes_from = edges.map(lambda x: x[0])\n",
    "nodes_to = edges.map(lambda x: x[1])\n",
    "all_nodes = nodes_from.union(nodes_to).distinct().cache()\n",
    "\n",
    "\n",
    "start_node = 0 \n",
    "infinity = float('inf')\n",
    "distances = all_nodes.map(lambda node: (node, infinity))\n",
    "distances = distances.map(lambda x: (x[0], 0.0) if x[0] == start_node else x)\n",
    "distances = distances.cache()\n",
    "\n",
    "\n",
    "updated = True\n",
    "iteration = 0\n",
    "max_iterations = all_nodes.count() - 1 \n",
    "\n",
    "while updated and iteration < max_iterations:\n",
    "    iteration += 1\n",
    "    joined = distances.join(adjacency_list, numPartitions=8)\n",
    "    \n",
    "    tentative_distances = joined.flatMap(lambda x: [ \n",
    "        (neighbor[0], x[1][0] + neighbor[1]) for neighbor in x[1][1]\n",
    "    ])\n",
    "\n",
    "    new_distances = distances.union(tentative_distances) \\\n",
    "                             .reduceByKey(lambda x, y: min(x, y))\n",
    "    \n",
    "    changes = new_distances.join(distances).filter(lambda x: x[1][0] != x[1][1])\n",
    "    updated = not changes.isEmpty()\n",
    "    \n",
    "    distances = new_distances\n",
    "    distances = distances.cache()\n",
    "\n",
    "\n",
    "final_distances = distances.collectAsMap()\n",
    "\n",
    "\n",
    "with open('output_2.txt', 'w') as f:\n",
    "    for node in sorted(final_distances.keys()):\n",
    "        dist = final_distances[node]\n",
    "        if dist == infinity:\n",
    "            f.write(f\"{node} unreachable\\n\")\n",
    "        else:\n",
    "            f.write(f\"{node} {dist}\\n\")\n",
    "\n",
    "\n",
    "reachable_nodes = {node: dist for node, dist in final_distances.items() if dist != infinity and node != start_node}\n",
    "if reachable_nodes:\n",
    "\n",
    "    max_distance = max(reachable_nodes.values())\n",
    "    min_distance = min(reachable_nodes.values())\n",
    "    \n",
    "    max_nodes = [node for node, dist in reachable_nodes.items() if dist == max_distance]\n",
    "    min_nodes = [node for node, dist in reachable_nodes.items() if dist == min_distance]\n",
    "\n",
    "    print(f\"Nodes with greatest distance from {start_node} (Distance: {max_distance}): {max_nodes}\")\n",
    "    print(f\"Nodes with least distance from {start_node} (Distance: {min_distance}): {min_nodes}\")\n",
    "else:\n",
    "    print(\"No reachable nodes from the starting node.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b3fdda8-4ea1-4716-bcd7-5af5740bd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9976e6a-2813-4ff0-927d-3ca7853367be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540283d-86f3-4f86-ab04-c3622220ae1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

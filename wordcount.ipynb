{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba171e22-6ce9-472c-8bcc-54e9de653378",
   "metadata": {},
   "source": [
    "Assignment - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb8537-0597-4a59-849d-26785b569a5e",
   "metadata": {},
   "source": [
    "Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d0e88ed-ba1d-40f4-92f9-49a40d90b113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 12:07:23 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import shutil\n",
    "import os\n",
    "import string\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "output_path_1 = \"output_1\"\n",
    "output_path_2 = \"output_2\"\n",
    "\n",
    "# Remove the output directories if they already exist\n",
    "if os.path.exists(output_path_1):\n",
    "    shutil.rmtree(output_path_1)\n",
    "if os.path.exists(output_path_2):\n",
    "    shutil.rmtree(output_path_2)\n",
    "\n",
    "try:\n",
    "    # Initialize SparkSession\n",
    "    spark = SparkSession.builder.appName(\"WordCountApp\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "    # Get the SparkContext\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d198d29-b82d-413a-b343-979d64c1323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    text_file = sc.textFile(\"littlewoman.txt\")\n",
    "    text_file_2 = sc.textFile(\"pride_and_prejudice.txt\")\n",
    "\n",
    "    # Perform transformations and actions\n",
    "    counts = text_file.flatMap(lambda line: line.translate(str.maketrans(\"\", \"\", string.punctuation)).lower().split()) \\\n",
    "                      .filter(lambda word: word not in stop_words) \\\n",
    "                      .map(lambda word: (word, 1)) \\\n",
    "                      .reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1],ascending=False)\n",
    "\n",
    "    counts1 = text_file_2.flatMap(lambda line: line.translate(str.maketrans(\"\", \"\", string.punctuation)).lower().split()) \\\n",
    "                         .filter(lambda word: word not in stop_words) \\\n",
    "                         .map(lambda word: (word, 1)) \\\n",
    "                         .reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1],ascending=False)\n",
    "\n",
    "    # Save output as directories\n",
    "    counts.coalesce(1).saveAsTextFile(output_path_1)\n",
    "    counts1.coalesce(1).saveAsTextFile(output_path_2)\n",
    "\n",
    "    output1 = counts.collect()\n",
    "    output2 = counts.collect()\n",
    "\n",
    "    with open('output_1_n.txt','w') as f:\n",
    "        for (word, count) in output1:\n",
    "            f.write(f\"{word}: {count}\\n\")\n",
    "\n",
    "    with open('output_2_n.txt','w') as f:\n",
    "        for (word, count) in output2:\n",
    "            f.write(f\"{word}: {count}\\n\")\n",
    "\n",
    "    # Merge partition files into a single output file\n",
    "    with open(\"output_1.txt\", \"w\") as outfile:\n",
    "        for filename in sorted(os.listdir(output_path_1)):\n",
    "            if filename.startswith(\"part-\"):\n",
    "                with open(os.path.join(output_path_1, filename), \"r\") as infile:\n",
    "                    outfile.write(infile.read())\n",
    "                    \n",
    "    with open(\"output_2.txt\", \"w\") as outfile:\n",
    "        for filename in sorted(os.listdir(output_path_2)):\n",
    "            if filename.startswith(\"part-\"):\n",
    "                with open(os.path.join(output_path_2, filename), \"r\") as infile:\n",
    "                    outfile.write(infile.read())\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"Word count files saved successfully.\")\n",
    "except Exception as e:\n",
    "     print(f\"An error occurred: {e}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23743e9e-409d-4b01-be32-b2031269f29e",
   "metadata": {},
   "source": [
    "Implement Djikstras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc2be3f1-afc2-4c26-8665-2a7bb24264ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 12:07:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 104:================================================>      (39 + 5) / 44]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node with greatest distance: ('1,', inf)\n",
      "Node with least distance: ('1,', inf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DijkstraShortestPath\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Define a large number to represent infinity\n",
    "INFINITY = float('inf')\n",
    "\n",
    "# Load graph edges from both files\n",
    "edges1 = sc.textFile(\"question2_1.txt\")\n",
    "edges2 = sc.textFile(\"question2_2.txt\")\n",
    "\n",
    "# Parse each line into (source, destination, weight)\n",
    "edges = edges1.union(edges2).map(lambda line: line.split()).map(lambda parts: (parts[0], parts[1], float(parts[2])))\n",
    "\n",
    "# Specify the starting node\n",
    "start_node = 'A'  # Replace 'A' with the desired starting node\n",
    "\n",
    "# Initialize distances RDD with start node distance 0, others as infinity\n",
    "distances = edges.flatMap(lambda x: [(x[0], INFINITY), (x[1], INFINITY)]) \\\n",
    "                 .distinct() \\\n",
    "                 .map(lambda x: (x[0], 0 if x[0] == start_node else INFINITY))\n",
    "\n",
    "# Initialize edges RDD for graph representation\n",
    "graph = edges.map(lambda x: (x[0], (x[1], x[2])))\n",
    "\n",
    "# Iteratively update distances using Dijkstra's algorithm\n",
    "def dijkstra_update(distances, graph):\n",
    "    updated_distances = distances.join(graph) \\\n",
    "        .flatMap(lambda x: [(x[0], x[1][0]), (x[1][1][0], x[1][0] + x[1][1][1])]) \\\n",
    "        .reduceByKey(lambda a, b: min(a, b))\n",
    "    return updated_distances\n",
    "\n",
    "for _ in range(10):  # Limit iterations for convergence; adjust as necessary\n",
    "    distances = dijkstra_update(distances, graph)\n",
    "\n",
    "# Collect and save results to output_2.txt\n",
    "shortest_paths = distances.collect()\n",
    "with open(\"output_3.txt\", \"w\") as f:\n",
    "    for node, dist in shortest_paths:\n",
    "        f.write(f\"{node}: {dist}\\n\")\n",
    "\n",
    "# Find nodes with the greatest and least distance\n",
    "max_node = max(shortest_paths, key=lambda x: x[1])\n",
    "min_node = min(shortest_paths, key=lambda x: x[1] if x[1] > 0 else sys.maxsize)\n",
    "\n",
    "print(f\"Node with greatest distance: {max_node}\")\n",
    "print(f\"Node with least distance: {min_node}\")\n",
    "\n",
    "# Stop Spark session\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518d851-8157-4304-bbc2-1f26c0eae875",
   "metadata": {},
   "source": [
    "Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b237dd10-7e3e-496d-92fc-ae2f01206069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 12:14:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 136:==========================================>            (17 + 5) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page with highest rank: ('50', 4.176179975971073)\n",
      "Page with lowest rank: ('35', 0.2593061945644343)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PageRank\").master([\"local[*]\"]).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load and parse the network file\n",
    "lines = sc.textFile(\"question3.txt\")\n",
    "\n",
    "# Parse each line into (page, neighbors) pairs\n",
    "def parse_neighbors(line):\n",
    "    parts = re.split(r':\\s*\\[|\\]', line)\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    page = parts[0].strip()\n",
    "    neighbors = parts[1].strip().split(', ')\n",
    "    return page, neighbors\n",
    "\n",
    "# Create an RDD of (page, list of neighbors)\n",
    "links = lines.map(parse_neighbors).filter(lambda x: x is not None)\n",
    "\n",
    "# Initialize each page's rank to 1.0\n",
    "ranks = links.mapValues(lambda _: 1.0)\n",
    "\n",
    "# Number of iterations for convergence\n",
    "iterations = 10\n",
    "damping_factor = 0.85  # Damping factor for PageRank\n",
    "\n",
    "# Run PageRank algorithm for a fixed number of iterations\n",
    "for _ in range(iterations):\n",
    "    # Calculate contributions for each page\n",
    "    contributions = links.join(ranks).flatMap(\n",
    "        lambda page_neighbors_rank: [(neighbor, page_neighbors_rank[1][1] / len(page_neighbors_rank[1][0])) \n",
    "                                     for neighbor in page_neighbors_rank[1][0]]\n",
    "    )\n",
    "    \n",
    "    # Calculate new ranks based on contributions\n",
    "    ranks = contributions.reduceByKey(lambda a, b: a + b).mapValues(\n",
    "        lambda rank: (1 - damping_factor) + damping_factor * rank\n",
    "    )\n",
    "\n",
    "# Collect and save the final ranks to output file\n",
    "page_ranks = ranks.collect()\n",
    "with open(\"output_page_ranks.txt\", \"w\") as f:\n",
    "    for page, rank in page_ranks:\n",
    "        f.write(f\"{page}: {rank}\\n\")\n",
    "\n",
    "# Find the page with the highest and lowest PageRank\n",
    "max_page = max(page_ranks, key=lambda x: x[1])\n",
    "min_page = min(page_ranks, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Page with highest rank: {max_page}\")\n",
    "print(f\"Page with lowest rank: {min_page}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdffe6a-624f-4758-92e8-6015d7c62b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

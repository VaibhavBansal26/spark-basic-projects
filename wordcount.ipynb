{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba171e22-6ce9-472c-8bcc-54e9de653378",
   "metadata": {},
   "source": [
    "Assignment - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb8537-0597-4a59-849d-26785b569a5e",
   "metadata": {},
   "source": [
    "Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0e88ed-ba1d-40f4-92f9-49a40d90b113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/10 17:57:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import explode, split, col, desc\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import shutil\n",
    "import os\n",
    "import string\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "output_path_1 = \"output_1\"\n",
    "\n",
    "# Remove the output directories if they already exist\n",
    "if os.path.exists(output_path_1):\n",
    "    shutil.rmtree(output_path_1)\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"WordCountApp\").master(\"local[*]\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d198d29-b82d-413a-b343-979d64c1323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|value             |\n",
      "+------------------+\n",
      "|('jo', 1293)      |\n",
      "|('said', 1245)    |\n",
      "|('one', 1159)     |\n",
      "|('mr', 1123)      |\n",
      "|('little', 961)   |\n",
      "|('would', 929)    |\n",
      "|('could', 893)    |\n",
      "|('much', 704)     |\n",
      "|('like', 676)     |\n",
      "|('meg', 653)      |\n",
      "|('mrs', 606)      |\n",
      "|('never', 605)    |\n",
      "|('elizabeth', 601)|\n",
      "|('amy', 588)      |\n",
      "|('see', 574)      |\n",
      "|('good', 572)     |\n",
      "|('laurie', 564)   |\n",
      "|('well', 557)     |\n",
      "|('know', 557)     |\n",
      "|('dont', 552)     |\n",
      "|('time', 522)     |\n",
      "|('go', 501)       |\n",
      "|('think', 496)    |\n",
      "|('must', 462)     |\n",
      "|('away', 453)     |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    text_file = sc.textFile(\"littlewoman.txt\")\n",
    "    text_file_2 = sc.textFile(\"pride_and_prejudice.txt\")\n",
    "    \n",
    "    count_combined = text_file.union(text_file_2)\n",
    "    count_combined_transformation = count_combined.flatMap(lambda line: line.translate(str.maketrans(\"\", \"\", string.punctuation)).lower().split())\n",
    "    count_combined_filter= count_combined_transformation.filter(lambda word: word not in stop_words)\n",
    "    count_combined_mapping = count_combined_filter.map(lambda word: (word, 1))\n",
    "    count_combined_unique = count_combined_mapping.reduceByKey(lambda a, b: a + b)\n",
    "    count_combined_sorted = count_combined_unique.sortBy(lambda x: x[1],ascending=False)\n",
    "\n",
    "    count_combined_sorted.saveAsTextFile(output_path_1)\n",
    "\n",
    "    # Merge partition files into a single output file\n",
    "\n",
    "    with open(\"output_1.txt\", \"w\") as outfile:\n",
    "        for filename in sorted(os.listdir(output_path_1)):\n",
    "            if filename.startswith(\"part-\"):\n",
    "                with open(os.path.join(output_path_1, filename), \"r\") as infile:\n",
    "                    outfile.write(infile.read())\n",
    "    \n",
    "    text_df = spark.read.text(\"output_1.txt\")\n",
    "\n",
    "    top_25_words = text_df.limit(25)\n",
    "    top_25_words.show(25, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "     print(f\"An error occurred: {e}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86216853-ada6-40b6-bd8f-f49978431181",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea79af1e-1d55-4d1c-a02c-ac1c5f4a2902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/10 17:58:36 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 18:===========================================>            (17 + 5) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page with highest rank: ('50', 4.176179975971073)\n",
      "Page with lowest rank: ('35', 0.2593061945644343)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PageRank\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load and parse the network file\n",
    "lines = sc.textFile(\"question3.txt\")\n",
    "\n",
    "# Parse each line into (page, neighbors) pairs\n",
    "def parse_neighbors(line):\n",
    "    parts = re.split(r':\\s*\\[|\\]', line)\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    page = parts[0].strip()\n",
    "    neighbors = parts[1].strip().split(', ')\n",
    "    return page, neighbors\n",
    "\n",
    "# Create an RDD of (page, list of neighbors)\n",
    "links = lines.map(parse_neighbors).filter(lambda x: x is not None)\n",
    "\n",
    "# Initialize each page's rank to 1.0\n",
    "ranks = links.mapValues(lambda _: 1.0)\n",
    "\n",
    "# Number of iterations for convergence\n",
    "iterations = 10\n",
    "damping_factor = 0.85  # Damping factor for PageRank\n",
    "\n",
    "# Run PageRank algorithm for a fixed number of iterations\n",
    "for _ in range(iterations):\n",
    "    # Calculate contributions for each page\n",
    "    contributions = links.join(ranks).flatMap(\n",
    "        lambda page_neighbors_rank: [(neighbor, page_neighbors_rank[1][1] / len(page_neighbors_rank[1][0])) \n",
    "                                     for neighbor in page_neighbors_rank[1][0]]\n",
    "    )\n",
    "    \n",
    "    # Calculate new ranks based on contributions\n",
    "    ranks = contributions.reduceByKey(lambda a, b: a + b).mapValues(\n",
    "        lambda rank: (1 - damping_factor) + damping_factor * rank\n",
    "    )\n",
    "\n",
    "# Collect and save the final ranks to output file\n",
    "page_ranks = ranks.collect()\n",
    "with open(\"output_page_ranks.txt\", \"w\") as f:\n",
    "    for page, rank in page_ranks:\n",
    "        f.write(f\"{page}: {rank}\\n\")\n",
    "\n",
    "# Find the page with the highest and lowest PageRank\n",
    "max_page = max(page_ranks, key=lambda x: x[1])\n",
    "min_page = min(page_ranks, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Page with highest rank: {max_page}\")\n",
    "print(f\"Page with lowest rank: {min_page}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d489ecc-407f-496e-9903-2ab332a88a39",
   "metadata": {},
   "source": [
    "Djikstra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32cf0f17-712b-4876-892f-9faf252f3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with greatest distance from 0 (Distance: 3.0): [32, 2, 35, 11, 13, 15, 51, 20, 90]\n",
      "Nodes with least distance from 0 (Distance: 1.0): [1, 66, 6, 7, 39, 71, 40, 9, 41, 75, 43, 76, 14, 16, 49, 19, 53, 54, 87, 57, 28, 60, 92]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import explode, split, col, desc\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkContext\n",
    "spark = SparkSession.builder.appName(\"Djikstra\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# Read the data files\n",
    "edges_rdd = sc.textFile('question2_1.txt').union(sc.textFile('question2_2.txt'))\n",
    "\n",
    "# Parse the edges into (source_node, destination_node, weight)\n",
    "edges = edges_rdd.map(lambda line: line.strip().split(',')) \\\n",
    "                 .map(lambda parts: (int(parts[0]), int(parts[1]), float(parts[2])))\n",
    "\n",
    "# Create an adjacency list RDD: (node, list of (neighbor, weight))\n",
    "adjacency_list = edges.map(lambda x: (x[0], (x[1], x[2]))) \\\n",
    "                      .groupByKey() \\\n",
    "                      .mapValues(list) \\\n",
    "                      .cache()\n",
    "\n",
    "# Get all nodes\n",
    "nodes_from = edges.map(lambda x: x[0])\n",
    "nodes_to = edges.map(lambda x: x[1])\n",
    "all_nodes = nodes_from.union(nodes_to).distinct().cache()\n",
    "\n",
    "# Initialize distances: (node, distance)\n",
    "start_node = 0  # Assuming the first node is 0\n",
    "infinity = float('inf')\n",
    "distances = all_nodes.map(lambda node: (node, infinity))\n",
    "distances = distances.map(lambda x: (x[0], 0.0) if x[0] == start_node else x)\n",
    "distances = distances.cache()\n",
    "\n",
    "# Iterative update of distances\n",
    "updated = True\n",
    "iteration = 0\n",
    "max_iterations = all_nodes.count() - 1  # Maximum possible iterations\n",
    "\n",
    "while updated and iteration < max_iterations:\n",
    "    iteration += 1\n",
    "    # Join distances with adjacency list\n",
    "    joined = distances.join(adjacency_list, numPartitions=8)\n",
    "    \n",
    "    # Compute tentative distances\n",
    "    tentative_distances = joined.flatMap(lambda x: [ \n",
    "        (neighbor[0], x[1][0] + neighbor[1]) for neighbor in x[1][1]\n",
    "    ])\n",
    "    \n",
    "    # Combine the new distances with the existing ones\n",
    "    new_distances = distances.union(tentative_distances) \\\n",
    "                             .reduceByKey(lambda x, y: min(x, y))\n",
    "    \n",
    "    # Check if distances have changed\n",
    "    changes = new_distances.join(distances).filter(lambda x: x[1][0] != x[1][1])\n",
    "    updated = not changes.isEmpty()\n",
    "    \n",
    "    # Update distances for the next iteration\n",
    "    distances = new_distances\n",
    "    distances = distances.cache()\n",
    "\n",
    "# Collect final distances\n",
    "final_distances = distances.collectAsMap()\n",
    "\n",
    "# Write distances to output file\n",
    "with open('output_2.txt', 'w') as f:\n",
    "    for node in sorted(final_distances.keys()):\n",
    "        dist = final_distances[node]\n",
    "        if dist == infinity:\n",
    "            f.write(f\"{node} unreachable\\n\")\n",
    "        else:\n",
    "            f.write(f\"{node} {dist}\\n\")\n",
    "\n",
    "# Find nodes with greatest and least distances (excluding infinity and the start node)\n",
    "reachable_nodes = {node: dist for node, dist in final_distances.items() if dist != infinity and node != start_node}\n",
    "if reachable_nodes:\n",
    "    # Find the maximum and minimum distances\n",
    "    max_distance = max(reachable_nodes.values())\n",
    "    min_distance = min(reachable_nodes.values())\n",
    "    \n",
    "    # Find all nodes that have the maximum and minimum distances\n",
    "    max_nodes = [node for node, dist in reachable_nodes.items() if dist == max_distance]\n",
    "    min_nodes = [node for node, dist in reachable_nodes.items() if dist == min_distance]\n",
    "    \n",
    "    # Print nodes with greatest distance\n",
    "    print(f\"Nodes with greatest distance from {start_node} (Distance: {max_distance}): {max_nodes}\")\n",
    "    \n",
    "    # Print nodes with least distance\n",
    "    print(f\"Nodes with least distance from {start_node} (Distance: {min_distance}): {min_nodes}\")\n",
    "else:\n",
    "    print(\"No reachable nodes from the starting node.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfbe723-4a5d-4716-b781-d1c0ce77f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518d851-8157-4304-bbc2-1f26c0eae875",
   "metadata": {},
   "source": [
    "Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdffe6a-624f-4758-92e8-6015d7c62b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
